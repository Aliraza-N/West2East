{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "behind-theme",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:32: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:38: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2.4.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorflow) (3.15.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (1.27.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (49.6.0.post20210108)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: keras in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from keras) (1.5.3)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from keras) (1.19.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('tsv.csv')\n",
    "\n",
    "df['date_of_infraction']= pd.to_datetime(df['date_of_infraction'])\n",
    "\n",
    "df['Count Date'] = df['Count Date'].str.replace('*', '').str.replace(',', '')\n",
    "\n",
    "df['Latitude'] = df['Latitude'].str.replace('*', '').str.replace(',', '')\n",
    "\n",
    "df['Longitude'] = df['Longitude'].str.replace('*', '').str.replace(',', '')\n",
    "\n",
    "df['Longitude'] = df['Longitude'].str.replace('*', '').str.replace(',', '')\n",
    "\n",
    "df['8 Peak Hr Vehicle Volume'] = df['8 Peak Hr Vehicle Volume'].str.replace('*', '').str.replace(',', '')\n",
    "df['8 Peak Hr Pedestrian Volume'] = df['8 Peak Hr Pedestrian Volume'].str.replace('*', '').str.replace(',', '')\n",
    "df['Activation Date'] = df['Activation Date'].str.replace('*', '').str.replace(',', '')\n",
    "df['TCS '] = df['TCS '].str.replace('*', '').str.replace(',', '')\n",
    "\n",
    "df['Count Date']= pd.to_datetime(df['Count Date'])\n",
    "\n",
    "df['Activation Date']= pd.to_datetime(df['Activation Date'])\n",
    "\n",
    "df\n",
    "\n",
    "df1 = df[['date_of_infraction','set_fine_amount','time_of_infraction','location2','Latitude','Longitude','Count Date','8 Peak Hr Vehicle Volume','8 Peak Hr Pedestrian Volume']]\n",
    "\n",
    "df1\n",
    "\n",
    "df1['date_of_infraction_year'] = df1['date_of_infraction'].dt.year\n",
    "df1['date_of_infraction_month'] = df1['date_of_infraction'].dt.month\n",
    "df1['date_of_infraction_week'] = df1['date_of_infraction'].dt.week\n",
    "df1['date_of_infraction_day'] = df1['date_of_infraction'].dt.day\n",
    "df1['date_of_infraction_dayofweek'] = df1['date_of_infraction'].dt.dayofweek\n",
    "\n",
    "df1['Count Date_year'] = df1['Count Date'].dt.year\n",
    "df1['Count Date_month'] = df1['Count Date'].dt.month\n",
    "df1['Count Date_week'] = df1['Count Date'].dt.week\n",
    "df1['Count Date_day'] = df1['Count Date'].dt.day\n",
    "df1['Count Date_dayofweek'] = df1['Count Date'].dt.dayofweek\n",
    "\n",
    "df1\n",
    "\n",
    "df1 = df1.drop('date_of_infraction',axis = 1)\n",
    "df1 = df1.drop('Count Date',axis = 1)\n",
    "df1 = df1.drop('location2',axis = 1)\n",
    "\n",
    "\n",
    "df1 = df1.dropna(how='all')\n",
    "\n",
    "df1[\"8 Peak Hr Vehicle Volume\"] = df1[\"8 Peak Hr Vehicle Volume\"].astype(str).astype(int)\n",
    "\n",
    "df1[\"8 Peak Hr Pedestrian Volume\"] = df1[\"8 Peak Hr Pedestrian Volume\"].astype(str).astype(int)\n",
    "\n",
    "df1['hour_sin'] = np.sin(2 * np.pi * df1['time_of_infraction']/23.0)\n",
    "\n",
    "\n",
    "df1['hour_cos'] = np.cos(2 * np.pi * df1['time_of_infraction']/23.0)\n",
    "\n",
    "\n",
    "df1[\"Latitude\"] = df1.Latitude.astype(float)\n",
    "\n",
    "\n",
    "df1[\"Longitude\"] = df1.Latitude.astype(float)\n",
    "\n",
    "\n",
    "np.where(df1.values >= np.finfo(np.float64).max)\n",
    "\n",
    "\n",
    "df1 = df1.dropna() \n",
    "\n",
    "pd.isnull(df1).sum() > 0\n",
    "\n",
    "X=df1.drop(columns=['8 Peak Hr Vehicle Volume','8 Peak Hr Pedestrian Volume'])\n",
    "y= df1[['8 Peak Hr Vehicle Volume','8 Peak Hr Pedestrian Volume']]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.60)\n",
    "\n",
    "!pip install tensorflow\n",
    "\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charming-joining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['set_fine_amount', 'time_of_infraction', 'Latitude', 'Longitude',\n",
       "       '8 Peak Hr Vehicle Volume', '8 Peak Hr Pedestrian Volume',\n",
       "       'date_of_infraction_year', 'date_of_infraction_month',\n",
       "       'date_of_infraction_week', 'date_of_infraction_day',\n",
       "       'date_of_infraction_dayofweek', 'Count Date_year', 'Count Date_month',\n",
       "       'Count Date_week', 'Count Date_day', 'Count Date_dayofweek', 'hour_sin',\n",
       "       'hour_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alert-brunei",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating bins for different sets\n",
    "bin_names1=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O']\n",
    "df1['Level']=pd.qcut(df1['time_of_infraction'],15,labels=bin_names1)\n",
    "#Plotting number of datapoints in each bin.\n",
    "df1['Level'].value_counts().plot(kind='barh')\n",
    "E=df1.loc[df1['Level']=='G']\n",
    "E=E.drop(columns='Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "annoying-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=E.drop(columns=['8 Peak Hr Vehicle Volume','8 Peak Hr Pedestrian Volume'])\n",
    "y= E[['8 Peak Hr Vehicle Volume','8 Peak Hr Pedestrian Volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "postal-expansion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set_fine_amount</th>\n",
       "      <th>time_of_infraction</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>8 Peak Hr Vehicle Volume</th>\n",
       "      <th>8 Peak Hr Pedestrian Volume</th>\n",
       "      <th>date_of_infraction_year</th>\n",
       "      <th>date_of_infraction_month</th>\n",
       "      <th>date_of_infraction_week</th>\n",
       "      <th>date_of_infraction_day</th>\n",
       "      <th>date_of_infraction_dayofweek</th>\n",
       "      <th>Count Date_year</th>\n",
       "      <th>Count Date_month</th>\n",
       "      <th>Count Date_week</th>\n",
       "      <th>Count Date_day</th>\n",
       "      <th>Count Date_dayofweek</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>30</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>1982</td>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>-3.916394e-15</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>50</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>43.775760</td>\n",
       "      <td>43.775760</td>\n",
       "      <td>13825</td>\n",
       "      <td>534</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2.697968e-01</td>\n",
       "      <td>0.962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>40</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>1982</td>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>2.697968e-01</td>\n",
       "      <td>0.962917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>40</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>1982</td>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>5.195840e-01</td>\n",
       "      <td>0.854419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>30</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>1982</td>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>8.878852e-01</td>\n",
       "      <td>0.460065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954192</th>\n",
       "      <td>50</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>1982</td>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>9.422609e-01</td>\n",
       "      <td>-0.334880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954193</th>\n",
       "      <td>40</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>43.636616</td>\n",
       "      <td>43.636616</td>\n",
       "      <td>5781</td>\n",
       "      <td>1156</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>9.422609e-01</td>\n",
       "      <td>-0.334880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954194</th>\n",
       "      <td>50</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>1982</td>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>9.422609e-01</td>\n",
       "      <td>-0.334880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954195</th>\n",
       "      <td>50</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>1982</td>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>9.422609e-01</td>\n",
       "      <td>-0.334880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954196</th>\n",
       "      <td>100</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>43.818187</td>\n",
       "      <td>1982</td>\n",
       "      <td>16</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>9.422609e-01</td>\n",
       "      <td>-0.334880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64024 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        set_fine_amount  time_of_infraction   Latitude  Longitude  \\\n",
       "1257                 30              1035.0  43.818187  43.818187   \n",
       "1258                 50              1036.0  43.775760  43.775760   \n",
       "1259                 40              1036.0  43.818187  43.818187   \n",
       "1260                 40              1037.0  43.818187  43.818187   \n",
       "1261                 30              1039.0  43.818187  43.818187   \n",
       "...                 ...                 ...        ...        ...   \n",
       "954192               50              1134.0  43.818187  43.818187   \n",
       "954193               40              1134.0  43.636616  43.636616   \n",
       "954194               50              1134.0  43.818187  43.818187   \n",
       "954195               50              1134.0  43.818187  43.818187   \n",
       "954196              100              1134.0  43.818187  43.818187   \n",
       "\n",
       "        8 Peak Hr Vehicle Volume  8 Peak Hr Pedestrian Volume  \\\n",
       "1257                        1982                           16   \n",
       "1258                       13825                          534   \n",
       "1259                        1982                           16   \n",
       "1260                        1982                           16   \n",
       "1261                        1982                           16   \n",
       "...                          ...                          ...   \n",
       "954192                      1982                           16   \n",
       "954193                      5781                         1156   \n",
       "954194                      1982                           16   \n",
       "954195                      1982                           16   \n",
       "954196                      1982                           16   \n",
       "\n",
       "        date_of_infraction_year  date_of_infraction_month  \\\n",
       "1257                       1970                         1   \n",
       "1258                       1970                         1   \n",
       "1259                       1970                         1   \n",
       "1260                       1970                         1   \n",
       "1261                       1970                         1   \n",
       "...                         ...                       ...   \n",
       "954192                     1970                         1   \n",
       "954193                     1970                         1   \n",
       "954194                     1970                         1   \n",
       "954195                     1970                         1   \n",
       "954196                     1970                         1   \n",
       "\n",
       "        date_of_infraction_week  date_of_infraction_day  \\\n",
       "1257                          1                       1   \n",
       "1258                          1                       1   \n",
       "1259                          1                       1   \n",
       "1260                          1                       1   \n",
       "1261                          1                       1   \n",
       "...                         ...                     ...   \n",
       "954192                        1                       1   \n",
       "954193                        1                       1   \n",
       "954194                        1                       1   \n",
       "954195                        1                       1   \n",
       "954196                        1                       1   \n",
       "\n",
       "        date_of_infraction_dayofweek  Count Date_year  Count Date_month  \\\n",
       "1257                               3             2009                 7   \n",
       "1258                               3             2016                 3   \n",
       "1259                               3             2009                 7   \n",
       "1260                               3             2009                 7   \n",
       "1261                               3             2009                 7   \n",
       "...                              ...              ...               ...   \n",
       "954192                             3             2009                 7   \n",
       "954193                             3             2012                 2   \n",
       "954194                             3             2009                 7   \n",
       "954195                             3             2009                 7   \n",
       "954196                             3             2009                 7   \n",
       "\n",
       "        Count Date_week  Count Date_day  Count Date_dayofweek      hour_sin  \\\n",
       "1257                 30              26                     6 -3.916394e-15   \n",
       "1258                 12              21                     0  2.697968e-01   \n",
       "1259                 30              26                     6  2.697968e-01   \n",
       "1260                 30              26                     6  5.195840e-01   \n",
       "1261                 30              26                     6  8.878852e-01   \n",
       "...                 ...             ...                   ...           ...   \n",
       "954192               30              26                     6  9.422609e-01   \n",
       "954193                8              22                     2  9.422609e-01   \n",
       "954194               30              26                     6  9.422609e-01   \n",
       "954195               30              26                     6  9.422609e-01   \n",
       "954196               30              26                     6  9.422609e-01   \n",
       "\n",
       "        hour_cos  \n",
       "1257    1.000000  \n",
       "1258    0.962917  \n",
       "1259    0.962917  \n",
       "1260    0.854419  \n",
       "1261    0.460065  \n",
       "...          ...  \n",
       "954192 -0.334880  \n",
       "954193 -0.334880  \n",
       "954194 -0.334880  \n",
       "954195 -0.334880  \n",
       "954196 -0.334880  \n",
       "\n",
       "[64024 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "driven-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the bin and obtaining the scores for the pca analysis.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Importing All major The Necessary Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "from sklearn.decomposition import PCA\n",
    "import  matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler \n",
    "E_index= E.index\n",
    "EE= StandardScaler().fit_transform(E)\n",
    "pca = PCA(n_components=3, svd_solver='full')\n",
    "PC_scores = pca.fit_transform(EE)\n",
    "scores_pd = pd.DataFrame(data = PC_scores\n",
    "                         ,columns = ['PC1', 'PC2', 'PC3']\n",
    "                         ,index =E_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preceding-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data = EE,columns = E.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "negative-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df2.drop(columns=['8 Peak Hr Vehicle Volume','8 Peak Hr Pedestrian Volume'])\n",
    "y= df2[['8 Peak Hr Vehicle Volume','8 Peak Hr Pedestrian Volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intense-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from numpy import array\n",
    "from numpy.random import uniform\n",
    "from numpy import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "animal-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 1\n",
    "test_size=0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "convertible-operations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06666667, 0.        , 0.85449815, ..., 1.        , 0.5       ,\n",
       "        1.        ],\n",
       "       [0.11111111, 0.01010101, 0.68883249, ..., 0.        , 0.6352136 ,\n",
       "        0.98137189],\n",
       "       [0.08888889, 0.01010101, 0.85449815, ..., 1.        , 0.6352136 ,\n",
       "        0.98137189],\n",
       "       ...,\n",
       "       [0.11111111, 1.        , 0.85449816, ..., 1.        , 0.97223134,\n",
       "        0.32943737],\n",
       "       [0.11111111, 1.        , 0.85449816, ..., 1.        , 0.97223134,\n",
       "        0.32943737],\n",
       "       [0.22222222, 1.        , 0.85449816, ..., 1.        , 0.97223134,\n",
       "        0.32943737]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#cleaning test data\n",
    "df = df2.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "df\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_scaled = scaler.fit_transform(df.values)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "exempt-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "exciting-palestine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape == (64023, 1, 17).\n",
      "y_train shape == (64023, 1).\n"
     ]
    }
   ],
   "source": [
    "# having the dataset as x and y . Making x into a 3-d data and y as 2-d data ie.) reshaping\n",
    "X_train = []\n",
    "y_train = []\n",
    "n_output_steps = 1  # Number of outputs we want to predict into the future\n",
    "n_input_steps = 1   # Number of past inputs that we want to use to predict the future\n",
    "for i in range(n_input_steps, len(df_scaled) - n_output_steps +1):\n",
    "    X_train.append(df_scaled[i - n_input_steps:i, 0:df.shape[1] - 1])\n",
    "    y_train.append(df_scaled[i + n_output_steps - 1:i + n_output_steps, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "print('X_train shape == {}.'.format(X_train.shape))   # no.of samples, no. of time stamps, no. of features\n",
    "print('y_train shape == {}.'.format(y_train.shape))   # no. of features, no. of output time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "short-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "correct-stock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set_fine_amount</th>\n",
       "      <th>time_of_infraction</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>date_of_infraction_year</th>\n",
       "      <th>date_of_infraction_month</th>\n",
       "      <th>date_of_infraction_week</th>\n",
       "      <th>date_of_infraction_day</th>\n",
       "      <th>date_of_infraction_dayofweek</th>\n",
       "      <th>Count Date_year</th>\n",
       "      <th>Count Date_month</th>\n",
       "      <th>Count Date_week</th>\n",
       "      <th>Count Date_day</th>\n",
       "      <th>Count Date_dayofweek</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.479602</td>\n",
       "      <td>-1.569059</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.703299</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.834460</td>\n",
       "      <td>-0.077559</td>\n",
       "      <td>1.222196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.071145</td>\n",
       "      <td>-1.540834</td>\n",
       "      <td>0.049231</td>\n",
       "      <td>0.049231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.788330</td>\n",
       "      <td>-1.143054</td>\n",
       "      <td>-1.148194</td>\n",
       "      <td>-0.128798</td>\n",
       "      <td>-1.902665</td>\n",
       "      <td>0.309235</td>\n",
       "      <td>1.169196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.275374</td>\n",
       "      <td>-1.540834</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.703299</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.834460</td>\n",
       "      <td>0.309235</td>\n",
       "      <td>1.169196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.275374</td>\n",
       "      <td>-1.512609</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.703299</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.834460</td>\n",
       "      <td>0.667342</td>\n",
       "      <td>1.014129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.479602</td>\n",
       "      <td>-1.456159</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.703299</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.834460</td>\n",
       "      <td>1.195357</td>\n",
       "      <td>0.450511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64019</th>\n",
       "      <td>-0.071145</td>\n",
       "      <td>1.225216</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.703299</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.834460</td>\n",
       "      <td>1.273313</td>\n",
       "      <td>-0.685639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64020</th>\n",
       "      <td>-0.275374</td>\n",
       "      <td>1.225216</td>\n",
       "      <td>-2.012774</td>\n",
       "      <td>-2.012774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.364542</td>\n",
       "      <td>-1.520184</td>\n",
       "      <td>-1.492992</td>\n",
       "      <td>0.015770</td>\n",
       "      <td>-0.990290</td>\n",
       "      <td>1.273313</td>\n",
       "      <td>-0.685639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64021</th>\n",
       "      <td>-0.071145</td>\n",
       "      <td>1.225216</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.703299</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.834460</td>\n",
       "      <td>1.273313</td>\n",
       "      <td>-0.685639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64022</th>\n",
       "      <td>-0.071145</td>\n",
       "      <td>1.225216</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.703299</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.834460</td>\n",
       "      <td>1.273313</td>\n",
       "      <td>-0.685639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64023</th>\n",
       "      <td>0.949997</td>\n",
       "      <td>1.225216</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.703299</td>\n",
       "      <td>0.365467</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.594042</td>\n",
       "      <td>0.834460</td>\n",
       "      <td>1.273313</td>\n",
       "      <td>-0.685639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64024 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       set_fine_amount  time_of_infraction  Latitude  Longitude  \\\n",
       "0            -0.479602           -1.569059  0.677966   0.677966   \n",
       "1            -0.071145           -1.540834  0.049231   0.049231   \n",
       "2            -0.275374           -1.540834  0.677966   0.677966   \n",
       "3            -0.275374           -1.512609  0.677966   0.677966   \n",
       "4            -0.479602           -1.456159  0.677966   0.677966   \n",
       "...                ...                 ...       ...        ...   \n",
       "64019        -0.071145            1.225216  0.677966   0.677966   \n",
       "64020        -0.275374            1.225216 -2.012774  -2.012774   \n",
       "64021        -0.071145            1.225216  0.677966   0.677966   \n",
       "64022        -0.071145            1.225216  0.677966   0.677966   \n",
       "64023         0.949997            1.225216  0.677966   0.677966   \n",
       "\n",
       "       date_of_infraction_year  date_of_infraction_month  \\\n",
       "0                          0.0                       0.0   \n",
       "1                          0.0                       0.0   \n",
       "2                          0.0                       0.0   \n",
       "3                          0.0                       0.0   \n",
       "4                          0.0                       0.0   \n",
       "...                        ...                       ...   \n",
       "64019                      0.0                       0.0   \n",
       "64020                      0.0                       0.0   \n",
       "64021                      0.0                       0.0   \n",
       "64022                      0.0                       0.0   \n",
       "64023                      0.0                       0.0   \n",
       "\n",
       "       date_of_infraction_week  date_of_infraction_day  \\\n",
       "0                          0.0                     0.0   \n",
       "1                          0.0                     0.0   \n",
       "2                          0.0                     0.0   \n",
       "3                          0.0                     0.0   \n",
       "4                          0.0                     0.0   \n",
       "...                        ...                     ...   \n",
       "64019                      0.0                     0.0   \n",
       "64020                      0.0                     0.0   \n",
       "64021                      0.0                     0.0   \n",
       "64022                      0.0                     0.0   \n",
       "64023                      0.0                     0.0   \n",
       "\n",
       "       date_of_infraction_dayofweek  Count Date_year  Count Date_month  \\\n",
       "0                               0.0        -0.703299          0.365467   \n",
       "1                               0.0         1.788330         -1.143054   \n",
       "2                               0.0        -0.703299          0.365467   \n",
       "3                               0.0        -0.703299          0.365467   \n",
       "4                               0.0        -0.703299          0.365467   \n",
       "...                             ...              ...               ...   \n",
       "64019                           0.0        -0.703299          0.365467   \n",
       "64020                           0.0         0.364542         -1.520184   \n",
       "64021                           0.0        -0.703299          0.365467   \n",
       "64022                           0.0        -0.703299          0.365467   \n",
       "64023                           0.0        -0.703299          0.365467   \n",
       "\n",
       "       Count Date_week  Count Date_day  Count Date_dayofweek  hour_sin  \\\n",
       "0             0.403399        0.594042              0.834460 -0.077559   \n",
       "1            -1.148194       -0.128798             -1.902665  0.309235   \n",
       "2             0.403399        0.594042              0.834460  0.309235   \n",
       "3             0.403399        0.594042              0.834460  0.667342   \n",
       "4             0.403399        0.594042              0.834460  1.195357   \n",
       "...                ...             ...                   ...       ...   \n",
       "64019         0.403399        0.594042              0.834460  1.273313   \n",
       "64020        -1.492992        0.015770             -0.990290  1.273313   \n",
       "64021         0.403399        0.594042              0.834460  1.273313   \n",
       "64022         0.403399        0.594042              0.834460  1.273313   \n",
       "64023         0.403399        0.594042              0.834460  1.273313   \n",
       "\n",
       "       hour_cos  \n",
       "0      1.222196  \n",
       "1      1.169196  \n",
       "2      1.169196  \n",
       "3      1.014129  \n",
       "4      0.450511  \n",
       "...         ...  \n",
       "64019 -0.685639  \n",
       "64020 -0.685639  \n",
       "64021 -0.685639  \n",
       "64022 -0.685639  \n",
       "64023 -0.685639  \n",
       "\n",
       "[64024 rows x 16 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cosmetic-international",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n",
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "#Variables\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "print(scaler_x.fit(X))\n",
    "xscale=scaler_x.transform(X)\n",
    "print(scaler_y.fit(y))\n",
    "yscale=scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "czech-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "parallel-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras import models, layers\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=16, kernel_initializer='uniform', activation='tanh'))\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "model.add(Dense(25, activation='tanh'))\n",
    "model.add(Dense(15, activation='tanh'))\n",
    "model.add(Dense(10, activation='tanh'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='tanh'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "close-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "posted-transparency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "241/241 [==============================] - 2s 4ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0506 - val_loss: 0.0058 - val_mse: 0.0058 - val_mae: 0.0437\n",
      "Epoch 2/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0438 - val_loss: 0.0056 - val_mse: 0.0056 - val_mae: 0.0433\n",
      "Epoch 3/100\n",
      "241/241 [==============================] - 1s 3ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0434 - val_loss: 0.0055 - val_mse: 0.0055 - val_mae: 0.0426\n",
      "Epoch 4/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0433 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0425\n",
      "Epoch 5/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0426 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0427\n",
      "Epoch 6/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0425 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0424\n",
      "Epoch 7/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0424 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0422\n",
      "Epoch 8/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0415 - val_loss: 0.0052 - val_mse: 0.0052 - val_mae: 0.0421\n",
      "Epoch 9/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0422 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0421\n",
      "Epoch 10/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0422 - val_loss: 0.0051 - val_mse: 0.0051 - val_mae: 0.0422\n",
      "Epoch 11/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0423 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0422\n",
      "Epoch 12/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0423 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0419\n",
      "Epoch 13/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0421 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0420\n",
      "Epoch 14/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0424 - val_loss: 0.0050 - val_mse: 0.0050 - val_mae: 0.0419\n",
      "Epoch 15/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0422 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0418\n",
      "Epoch 16/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0419 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0418\n",
      "Epoch 17/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0419 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0417\n",
      "Epoch 18/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0422 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0418\n",
      "Epoch 19/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0421 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0416\n",
      "Epoch 20/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0416 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0416\n",
      "Epoch 21/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0419 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0416\n",
      "Epoch 22/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0417 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0416\n",
      "Epoch 23/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0415 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0415\n",
      "Epoch 24/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0419 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0415\n",
      "Epoch 25/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0419 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0414\n",
      "Epoch 26/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0420 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0414\n",
      "Epoch 27/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0412 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0415\n",
      "Epoch 28/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0417 - val_loss: 0.0049 - val_mse: 0.0049 - val_mae: 0.0415\n",
      "Epoch 29/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0416 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0414\n",
      "Epoch 30/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0418 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0414\n",
      "Epoch 31/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0417 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0414\n",
      "Epoch 32/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0419 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0414\n",
      "Epoch 33/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0413 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0413\n",
      "Epoch 34/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0410 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 35/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0417 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0414\n",
      "Epoch 36/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 37/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0421 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 38/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 39/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0417 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 40/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 41/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0415 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0413\n",
      "Epoch 42/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0409 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 43/100\n",
      "241/241 [==============================] - 1s 3ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0419 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0414\n",
      "Epoch 44/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0417 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 45/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0419 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 46/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0414\n",
      "Epoch 47/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0416 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 48/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0416 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 49/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0418 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 50/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 51/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 52/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 53/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 54/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0416 - val_loss: 0.0048 - val_mse: 0.0048 - val_mae: 0.0413\n",
      "Epoch 55/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0415 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 56/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0411 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 57/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 58/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0417 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 59/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 60/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0417 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 61/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 62/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0412 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0413\n",
      "Epoch 63/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0413 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 64/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0410 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 65/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0413 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 66/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0413 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 67/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0418 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 68/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 69/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0411 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 70/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0417 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 71/100\n",
      "241/241 [==============================] - 1s 3ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 72/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0412 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 73/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0409 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 74/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0413 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 75/100\n",
      "241/241 [==============================] - 0s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0413 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 76/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0412 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 77/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0413 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 78/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 79/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 80/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 81/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0417 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 82/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 83/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0410 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 84/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 85/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0418 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 86/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 87/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0418 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0412\n",
      "Epoch 88/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 89/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 90/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0412 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 91/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 92/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0417 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0412\n",
      "Epoch 93/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0410 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0412\n",
      "Epoch 94/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0419 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 95/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 96/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0419 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0412\n",
      "Epoch 97/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0412 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0412\n",
      "Epoch 98/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0419 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0412\n",
      "Epoch 99/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0413 - val_loss: 0.0047 - val_mse: 0.0047 - val_mae: 0.0412\n",
      "Epoch 100/100\n",
      "241/241 [==============================] - 1s 2ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0410 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0412\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=160,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "three-adolescent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501/501 [==============================] - 0s 908us/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.004593718331307173, 0.004593718331307173, 0.04076879471540451]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "moved-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "differential-distinction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"tanh_LSTM_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "swedish-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Dense(300, activation='relu', input_shape=[X_train.shape[1]]))\n",
    "model1.add(layers.Dense(150, activation='relu'))\n",
    "model1.add(layers.Dense(75, activation='relu'))\n",
    "model1.add(layers.Dense(25, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='relu'))\n",
    "# output layer\n",
    "model1.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "gothic-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sensitive-magnitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1201/1201 [==============================] - 4s 2ms/step - loss: 0.0061 - mae: 0.0448 - val_loss: 0.0054 - val_mae: 0.0475\n",
      "Epoch 2/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0052 - mae: 0.0425 - val_loss: 0.0054 - val_mae: 0.0418\n",
      "Epoch 3/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0052 - mae: 0.0426 - val_loss: 0.0050 - val_mae: 0.0420\n",
      "Epoch 4/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0049 - mae: 0.0416 - val_loss: 0.0049 - val_mae: 0.0417\n",
      "Epoch 5/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0049 - mae: 0.0418 - val_loss: 0.0049 - val_mae: 0.0415\n",
      "Epoch 6/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0050 - mae: 0.0422 - val_loss: 0.0050 - val_mae: 0.0417\n",
      "Epoch 7/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0049 - mae: 0.0419 - val_loss: 0.0050 - val_mae: 0.0418\n",
      "Epoch 8/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0049 - mae: 0.0419 - val_loss: 0.0048 - val_mae: 0.0414\n",
      "Epoch 9/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0412 - val_loss: 0.0048 - val_mae: 0.0414\n",
      "Epoch 10/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0049 - mae: 0.0422 - val_loss: 0.0048 - val_mae: 0.0415\n",
      "Epoch 11/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0048 - mae: 0.0415 - val_loss: 0.0048 - val_mae: 0.0414\n",
      "Epoch 12/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0049 - mae: 0.0421 - val_loss: 0.0047 - val_mae: 0.0414\n",
      "Epoch 13/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0048 - mae: 0.0420 - val_loss: 0.0048 - val_mae: 0.0414\n",
      "Epoch 14/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0414\n",
      "Epoch 15/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 16/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0414\n",
      "Epoch 17/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0049 - mae: 0.0420 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 18/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0048 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0414\n",
      "Epoch 19/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 20/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0411 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 21/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 22/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0414\n",
      "Epoch 23/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0412 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 24/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0418 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 25/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 26/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0417 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 27/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 28/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 29/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0048 - val_mae: 0.0414\n",
      "Epoch 30/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0417 - val_loss: 0.0048 - val_mae: 0.0414\n",
      "Epoch 31/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0048 - mae: 0.0417 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 32/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 33/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 34/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 35/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 36/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0048 - mae: 0.0420 - val_loss: 0.0048 - val_mae: 0.0413\n",
      "Epoch 37/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 38/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 39/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0411 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 40/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0046 - mae: 0.0411 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 41/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0411\n",
      "Epoch 42/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 43/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0046 - mae: 0.0413 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 44/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 45/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 46/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 47/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 48/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0409 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 49/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 50/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 51/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0414\n",
      "Epoch 52/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 53/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 54/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0417 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 55/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0048 - mae: 0.0419 - val_loss: 0.0048 - val_mae: 0.0413\n",
      "Epoch 56/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0417 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 57/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0412 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 58/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0046 - mae: 0.0413 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 59/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0410 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 60/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0413 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 61/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 62/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 63/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 64/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0046 - mae: 0.0411 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 65/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 66/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 67/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0420 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 68/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0412 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 69/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 70/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0417 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 71/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 72/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0418 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 73/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0048 - mae: 0.0418 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 74/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0413\n",
      "Epoch 75/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 76/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0412 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 77/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0409 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 78/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 79/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0411\n",
      "Epoch 80/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 81/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 82/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0416 - val_loss: 0.0047 - val_mae: 0.0411\n",
      "Epoch 83/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 84/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0046 - mae: 0.0408 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 85/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 86/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 87/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0414 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 88/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 89/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 90/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0413 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 91/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0415 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 92/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0412 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 93/100\n",
      "1201/1201 [==============================] - 2s 2ms/step - loss: 0.0047 - mae: 0.0413 - val_loss: 0.0047 - val_mae: 0.0411\n",
      "Epoch 94/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0048 - mae: 0.0417 - val_loss: 0.0046 - val_mae: 0.0411\n",
      "Epoch 95/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0413 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 96/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0416 - val_loss: 0.0046 - val_mae: 0.0411\n",
      "Epoch 97/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0410 - val_loss: 0.0047 - val_mae: 0.0412\n",
      "Epoch 98/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0412 - val_loss: 0.0047 - val_mae: 0.0411\n",
      "Epoch 99/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0047 - mae: 0.0417 - val_loss: 0.0046 - val_mae: 0.0412\n",
      "Epoch 100/100\n",
      "1201/1201 [==============================] - 3s 2ms/step - loss: 0.0046 - mae: 0.0410 - val_loss: 0.0047 - val_mae: 0.0412\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train, y_train, validation_split=0.2, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "attached-vietnam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mae', 'val_loss', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "graduate-composite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501/501 [==============================] - 0s 960us/step - loss: 0.0046 - mae: 0.0408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.004587836563587189, 0.04076556861400604]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "regional-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = model1.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"RELU_LSTM_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "conscious-seventh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eli5\n",
      "  Downloading eli5-0.11.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from eli5) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from eli5) (0.24.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from eli5) (2.11.3)\n",
      "Requirement already satisfied: attrs>16.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from eli5) (20.3.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from eli5) (1.15.0)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.16-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from eli5) (0.8.9)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from eli5) (1.19.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-learn>=0.20->eli5) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from jinja2->eli5) (1.1.1)\n",
      "Installing collected packages: graphviz, eli5\n",
      "Successfully installed eli5-0.11.0 graphviz-0.16\n"
     ]
    }
   ],
   "source": [
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "regulated-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "previous-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    model2 = models.Sequential()\n",
    "    model2.add(layers.Dense(300, activation='relu', input_shape=[X_train.shape[1]]))\n",
    "    model2.add(layers.Dense(150, activation='relu'))\n",
    "    model2.add(layers.Dense(75, activation='relu'))\n",
    "    model2.add(layers.Dense(25, activation='relu'))\n",
    "    model2.add(layers.Dense(1, activation='relu'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "trying-surgery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0a8f7ddd68>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_model = KerasRegressor(build_fn=base_model, epochs=100, batch_size=16, verbose=0)    \n",
    "param_model.fit(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "constitutional-desperate",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f0a8cead898> does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-714de0f0f8a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPermutationImportance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/eli5/sklearn/permutation_importance.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \"\"\"\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpandas_available\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;34m\"If no scoring is specified, the estimator passed should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m\"have a 'score' method. The estimator %r does not.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 % estimator)\n\u001b[0m\u001b[1;32m    454\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         raise ValueError(\"For evaluating multiple scores, use \"\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f0a8cead898> does not."
     ]
    }
   ],
   "source": [
    "perm = PermutationImportance(model, random_state=1).fit(X_test,y_test)\n",
    "eli5.show_weights(perm, feature_names = X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-humor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
